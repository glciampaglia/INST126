{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba290c76-edbf-4749-92dc-b7734972d123",
   "metadata": {},
   "source": [
    "# Data Analysis with Pandas: split-apply-combine\n",
    "\n",
    "Last week, we learned:\n",
    "- Pandas is a library in Python that is designed for data manipulation and analysis\n",
    "- How to use libraries (import them, access their functions and data structures with `library.function_name()`)\n",
    "- About the `dataframe` data structure: basically a smart spreadsheet, with rows of observations, and columns of variables/data for each observation - sort of a cross between a list (sortable, indexable) and a dictionary (quickly access data by key)\n",
    "- Some basic operations: constructing a dataframe, summarizing, subsetting, reshaping\n",
    "\n",
    "This week, we'll learn a bit more about summarization:\n",
    "- Use `.value_counts()` to summarize categorical data\n",
    "- Use `.crosstab()` to summarize categorical data cross multiple columns\n",
    "\n",
    "And more advanced operations for reshaping/modifying your dataframe:\n",
    "- Use `.apply()` to apply functions to one or more columns to generate new columns\n",
    "- Use `.groupby()` to split your data into subgroups, apply some function to their data, then combine them into a new dataframe for further analysis (the \"**split-apply-combine**\" pattern that is fundamental to data analysis with pandas)\n",
    "- Use some basic plotting functions to explore your data\n",
    "\n",
    "I'll then tie it all together to show how they map to problem formulations for your Project 4: all the projects have the same basic structure! These roughly correspond to Qs 6-8 in your PCEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99f18e-c9aa-4751-aa14-98e4f51090dd",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The files we will be working with in this session are the datasets used for Project 4:\n",
    "\n",
    "<ul>\n",
    "    <li><button data-commandlinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/repos/INST-126-intro-to-programming-for-infosci/notebooks/data/bls-by-category.csv\", \"text\": \"testudo_fall2020.csv\"}'>View bls-by-category.csv</button>\n",
    "    </li>\n",
    "    <li><button data-commandlinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/repos/INST-126-intro-to-programming-for-infosci/notebooks/data/BreadBasket_DMS.csv\", \"text\": \"testudo_fall2020.csv\"}'>View BreadBasket_DMS.csv</button></li>\n",
    "    <li><button data-commandlinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/repos/INST-126-intro-to-programming-for-infosci/notebooks/data/ncaa-team-data.csv\", \"text\": \"testudo_fall2020.csv\"}'>View ncaa-team-data.csv</button></li>\n",
    "    <li><button data-commandlinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/repos/INST-126-intro-to-programming-for-infosci/notebooks/data/testudo_fall2020.csv\", \"text\": \"testudo_fall2020.csv\"}'>View testudo_fall2020.csv</button> (for this one, pass <code>keep_default_na=False</code> when using <code>read_csv</code>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328104d7-dbc1-49d9-bed5-665640c2f464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fn = 'data/testudo_fall2020.csv' # Note: on Windows use data\\testudo_fall2020.csv instead!\n",
    "\n",
    "# read in the file into a dataframe called courses\n",
    "courses = pd.read_csv(fn, keep_default_na=False)\n",
    "\n",
    "# use the .head() function to show the top 5 rows in the dataframe\n",
    "courses.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727caccb-a445-4582-b3d5-5ee316d36427",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is a quick summary of the dataframe, rounded to two decimal digits. Since `.describe()` works only on numerical columns, the only entry showed is the `credits` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211e8e7-d8d9-4d18-81fb-c26cace2b001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "courses.describe().round(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97973904-6a47-41f9-bd15-0a35292cfaee",
   "metadata": {},
   "source": [
    "## Use `.value_counts()` to summarize categorical data in your dataframe\n",
    "\n",
    "Another way to get a summary of one or more columns that are *categorical*. The counts correspond to how many time each particular _category_ (a value) appears in the column. Results are sorted in descending order by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9438e94-3fbd-49dd-8385-f137693094ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# access the area column in the courses dataframe\n",
    "area = courses['area']\n",
    "\n",
    "# and apply the value_counts method to that column, which is a series data structure\n",
    "area.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378235b7-0011-4fbe-865e-baaef9d2d370",
   "metadata": {},
   "source": [
    "Note that the result series is labeled with the possible values of the `area` column. (This is the major of the course.) As we saw last week, a Pandas series is like a cross between a dictionary and a list. So we can now use this new series to retrieve the counts for each individual area/major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631037d7-dc19-49a9-87bb-e134ad0e3fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same as above but stored in a new variable\n",
    "area_counts = courses['area'].value_counts()\n",
    "\n",
    "# can get value by named key like a dict\n",
    "area_code = \"INST\"\n",
    "print(f\"Count for {area_code=}: {area_counts[area_code]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8680d-cf2c-4999-a382-5b82b6710cce",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "We can also look up counts by integer position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541991a8-a97e-4458-aebe-aecc98870e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The first entry in the series\n",
    "print(f\"The first major has {area_counts.iloc[0]} courses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b287b-dc89-4d19-b9c6-3bf3eced70f9",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "However we don't know what major is that! To know what is the label associated to a particular integer position, we can make use of the `.index` attribute of a Pandas Series/Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdfd88-7202-4295-a2a9-9cf6b65b3a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and also by location\n",
    "lab0 = area_counts.index[0]\n",
    "val0 = area_counts.iloc[0]\n",
    "print(f\"The first major is {lab0} and has {val0} courses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5526f-cdae-43f1-849a-ed353ce7b42f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Note that `.value_counts()` returns results sorted in descending order, so the first entry is also the maximum. So the above could be achieved used the `.max()` / `.idxmax()` methods of pandas Series/Dataframe objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb58e4-c492-49c0-803d-c6584e0206a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"The major with most courses is {area_counts.idxmax()} with {area_counts.max()} courses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b5e07-080a-441e-a053-123040ee3103",
   "metadata": {},
   "source": [
    "<br/><br/><br/> To list all the keys use the `.keys()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd62b8-2718-4aff-8b39-f1583a538782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "area_counts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6514b-42ee-4d22-a636-1c0c49fcff2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/><br/>This is the same as the `.index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f0909-7e3d-4778-84e9-70be610ac043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "area_counts.keys() is area_counts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2dbf09-c171-46b2-bdfa-394c6465ff9c",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Let's say we want the top 5 most populous areas. We can slice/subset the series just like a list, and then get the keys from that subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01057c43-b46f-42b1-aa8a-1ddacb929d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a slice (like a list) and then get keys (like a dict)\n",
    "area_counts[:5].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f3178-6182-41a9-bb75-4669c5a3fcce",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a0964-c781-4145-ba28-768da9dfe7ac",
   "metadata": {},
   "source": [
    "## Coding Challenge \\#1\n",
    "\n",
    "How do we get the frequency counts for the items in the \"bread basket\" data frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af54bf-e24c-4fc5-9d45-9230022195e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83d5aa-7dad-433a-9ee2-0f554cf85d53",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce3d85-23e2-4685-9a6c-46d5a25a0fe1",
   "metadata": {},
   "source": [
    "## Use `crosstabs()` to summarize categorical data across multiple columns \n",
    "\n",
    "If we have _multiple_ categorical columns, we may want to get the frequency of a particular combination of values from both columns.\n",
    "\n",
    "Let's use the NCAA dataset to show this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288daffd-37f7-43c2-b9e6-3b6403b621ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data from CSV\n",
    "fn = \"data/ncaa-team-data.csv\"  # Note: on Windows use data\\ncaa-team-data.csv \n",
    "ncaa = pd.read_csv(fn, na_values=['NA'], keep_default_na=False)\n",
    "\n",
    "# summarize first 5 rows\n",
    "ncaa.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd7ade-d77c-431b-b464-be87a94a0c16",
   "metadata": {},
   "source": [
    "Let's explore the `ncaa_result` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461dc4b-bb72-4e10-ae01-77ff28e880fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All possible results\n",
    "ncaa['ncaa_result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a455c-6f99-4687-976e-514f42395bf5",
   "metadata": {},
   "source": [
    "Let's explore the `school` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409f6b2-59b9-4aef-a5f6-09271278649c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncaa['school'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8c46a-4a0e-4d51-8a6a-32b54475a2b8",
   "metadata": {},
   "source": [
    "What if I want to compute the counts of all possible combination of a school &times; a result? I could try to get the `.value_counts` of a sub-dataframe with these two columns only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f3413-d45e-4ce5-a143-5b3ed279fc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncaa[['school', 'ncaa_result']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054107d-bf64-4481-9e35-4eb223e6247a",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "This however gives me a series, not a dataframe. The `crosstab` function instead organizes the same data in a tabular format (so as a dataframe instead of a series). It takes two pandas Series as parameters, which are going to correspond to two components of the cross-tabulaation:\n",
    "1. The `index` parameter -- whose values are going to be the _rows_ of the cross-tab dataframe;\n",
    "2. The `column` parameter -- whose values are going to be the _columns_ of the cross-tab dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7eef92-d12a-4ba8-a8fd-c5a6d1d78a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label rows by the school, and columns by the season result \n",
    "pd.crosstab(ncaa['school'], ncaa['ncaa_result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380fd2f-1e8a-47ed-a006-a5bcf9dea846",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "Note that in the dataframe created by `pd.crosstab()` rows and columns are sorted in alphabetical order. While this may be OK for schools, it is definitely not the most intuitive for the various possible outcomes in the various NCAA tournaments.\n",
    "\n",
    "Can we reorganize the columns of a dataframe? There is a neat trick to do so, which relies on column _subsetting_ &mdash; the technique we saw last week that, given a dataframe, uses list indexing _on the columns_ to generate a new sub-dataframe having only a subset of the columns of the original dataframe. It turns out that the list indexing syntax _preserves the order of the columns_ in the sub-dataframe. So we can exploit this behavior to create a new &ldquo;sub&rdquo;-dataframe that has the _full_ set of columns, but in the order we want.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f960857-8e0e-43c7-98e3-076b582751a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The full set of columns -- sorted from most important to least important result\n",
    "NCAA_RESULTS_SORTED = [\n",
    "    \"Won National Final\",\n",
    "    \"Lost National Final\",\n",
    "    \"Lost National Semifinal\",\n",
    "    \"Lost Regional Final (Final Four)\",\n",
    "    \"Lost Regional Final\",\n",
    "    \"Lost Regional Semifinal\",\n",
    "    \"Lost Third Round\",\n",
    "    \"Lost Second Round\",\n",
    "    \"Lost First Round\",\n",
    "    \"Playing First Round\",\n",
    "    \"Lost First Four\",\n",
    "    \"Playing First Four\",\n",
    "    \"Lost Opening Round\"\n",
    "]\n",
    "\n",
    "# Label rows by the school, and columns by the season result, now sorted by descending outcome importance \n",
    "pd.crosstab(ncaa['school'], ncaa['ncaa_result'])[NCAA_RESULTS_SORTED]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938c2c3-d4e0-4d61-9453-4a174e8b768c",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6f7a2-be5a-4ce9-9ddb-af422c1a1093",
   "metadata": {},
   "source": [
    "## Cleaning data for cross-tabulation with `.apply()`\n",
    "\n",
    "Now that we know how cross-tabulation works, let's explore another column --- the `coaches` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71aeb0-f6a5-4f98-9f7d-c439a2d694fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All possible results\n",
    "ncaa['coaches'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a44905-a39a-405b-abff-83dda0b25ef3",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Now we would like to count how many times each coach had a particular result (column `ncaa_result`). One problem is that coaches can appear with different seasons (if they coached at different schools). Since we are interested in the performance of individual coaches, straight cross-tabulating the `coaches` column with `ncaa_result` would not work. \n",
    "\n",
    "So we need to _clean_ the `coaches` column to extract only the name without the season part. We can do this with a method of Pandas series/dataframes called `.apply()`. This requires us to define a &ldquo;cleaning&rdquo; function.\n",
    "\n",
    "Cleaning data is a laborious activity. It requires knowing all the various idiosyncracies of how the data were created. Click on the button below and scroll horizontally to the `coaches` column, and inspect a few of the entries. \n",
    "\n",
    "<button data-commandlinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/repos/INST-126-intro-to-programming-for-infosci/notebooks/23-Pandas-Split-Apply-Combine-Review/ncaa-team-data.csv\", \"text\": \"testudo_fall2020.csv\"}'>View ncaa-team-data.csv</button>\n",
    "\n",
    "If you looked through the data, you may have noticed a few quirks about how the information about coaches was recorded  in the NCAA data:\n",
    "1. Most entries follow the format: `<first_name> <last_name> (<year>-<year>)`;\n",
    "2. Some entries, however, include also a middle name: `<first_name> <middle_name> <last_name> (<year>-<year>)`;\n",
    "3. Some entries have multiple coaches, separated by comma: `<first_name> <last_name> (<year>-<year>), <first_name> <last_name> (<year>-<year>)`; for simplicity, in these cases we want to consider only the first coach;\n",
    "4. Finally, not all entries have a coach. Entries where the coach information is missing are recorded either as `\"Unknown\"`, `\"No coach\"`, or `\"None\"`; we want to keep those as is.\n",
    "\n",
    "So knowing these specific facts about our dataset, here is a function that will clean the `coaches` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d345e36-0d68-42f5-a8a2-914f34098dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coach_name(x):\n",
    "    \"\"\"\n",
    "    Extract the name of the (first) coach in the season\n",
    "    \"\"\"\n",
    "    # Return missing / unknown coaches as is\n",
    "    if x in ['Unknown', 'None', 'No coach']:\n",
    "        return x\n",
    "    \n",
    "    # If multiple coaches are listed, keep only the first one\n",
    "    if \",\" in x:\n",
    "        x = x.split(\",\")[0]\n",
    "        \n",
    "    # split the string around spaces to isolate first, (middle,) last name, and years\n",
    "    elements = x.split(' ')\n",
    "    \n",
    "    # take everything but the years (last element)\n",
    "    coach_name = elements[:-1]\n",
    "    \n",
    "    # recombine together the first, (middle,), and last name and return it.\n",
    "    coach_name = \" \".join(coach_name)\n",
    "    return coach_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8892bfb-1326-4d83-8d7c-babf3fc0846d",
   "metadata": {},
   "source": [
    "<br/><br/><br/>We can test that this function works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270ab85-1143-494e-86f5-c773419505d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Should return \"Bill Chandler\"\n",
    "get_coach_name(\"Bill Chandler (11-7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ece66-6912-4368-bc87-07d37229e367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Should return \"George Ireland\" only\n",
    "get_coach_name(\"George Ireland (8-6), Jerry Lyne (2-9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb8fcb-d302-4a26-97cc-fa78bcdbb34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Should return \"None\"\n",
    "get_coach_name(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c5081-e05e-46d3-91bc-071647b11a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Should return \"No coach\"\n",
    "get_coach_name(\"No coach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dec44-229e-4ca7-82d7-d4ee6505413f",
   "metadata": {},
   "source": [
    "<br/><br/><br/>To apply this function to all the entries in the colum, we pass *the function* to the `.apply()` method. This method repeatedly calls your function passing all entries in the column one at a time as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e80e1e-5f8a-47db-a300-2ef7b0e08b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncaa['coaches'].apply(get_coach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2eded8-8d27-481a-8453-c01286f181b6",
   "metadata": {},
   "source": [
    "<br/><br/><br/>We can now add this series as a cleaned column back into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab1af2-f871-4fd0-a1cc-288e35b694b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply get_coach_name function to each entry in column `coaches` and create new column `coach_name`\n",
    "ncaa['coach_name'] = ncaa['coaches'].apply(get_coach_name)\n",
    "\n",
    "# create a sub-dataframe with just two columns -- ncaa_result and coach_name\n",
    "pd.crosstab(ncaa['coach_name'], ncaa['ncaa_result'])[NCAA_RESULTS_SORTED]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d7a3d-46e4-4569-89de-db95634b8ab7",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "One advantage of having the data in tabular format is that now we can apply all the techniques for subsetting/reshaping the data that we know!\n",
    "\n",
    "For example, we can ask who are the people who won _some_ national finals but also lost _some_ regional finals. This uses subsetting with boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06859121-9f1f-4c3b-bbb3-f158ee0890fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store crosstab in a new variable\n",
    "coach_results = pd.crosstab(ncaa['coach_name'], ncaa['ncaa_result'])[NCAA_RESULTS_SORTED]\n",
    "\n",
    "# Use boolean indexing on two columns\n",
    "coach_results[(coach_results[\"Won National Final\"] > 0) & (coach_results['Lost Regional Final'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12065bf-0a78-4c24-b0fe-8b4d9a0b6080",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "It turns out that most of the people who won the national finals also lost a lot of regional finals, which tells us something about how difficult it is to win the nationals.\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1ec56-aef0-45b5-983b-268eb9a5f68e",
   "metadata": {},
   "source": [
    "## Coding Challenge \\#2\n",
    "\n",
    "Now do the same with the UMD courses data. Let's see how many areas offer introductory courses. Follow these steps:\n",
    "\n",
    "1. To decide whether a course is an &ldquo;introductory&rdquo; one, define a function that checks if a title string contains the word `\"Introduction\"`.\n",
    "2. Then apply the function to the `title` column and add the result as a new column in the dataframe.\n",
    "3. Finally, cross-tabulate the area by this new column. Store the result in a variables called `intro_by_area`.\n",
    "\n",
    "If all goes well this is the dataframe you should obtain:\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>is_intro</th>\n",
    "      <th>False</th>\n",
    "      <th>True</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>area</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>AMST</th>\n",
    "      <td>6</td>\n",
    "      <td>3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>BMGT</th>\n",
    "      <td>51</td>\n",
    "      <td>2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>CMSC</th>\n",
    "      <td>36</td>\n",
    "      <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>COMM</th>\n",
    "      <td>31</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ECON</th>\n",
    "      <td>63</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ENSP</th>\n",
    "      <td>4</td>\n",
    "      <td>2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ENTS</th>\n",
    "      <td>4</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>INFM</th>\n",
    "      <td>4</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>INST</th>\n",
    "      <td>40</td>\n",
    "      <td>7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MATH</th>\n",
    "      <td>40</td>\n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>PHSC</th>\n",
    "      <td>5</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>PLCY</th>\n",
    "      <td>28</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>PSYC</th>\n",
    "      <td>34</td>\n",
    "      <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>SPHL</th>\n",
    "      <td>7</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>STAT</th>\n",
    "      <td>11</td>\n",
    "      <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>URSP</th>\n",
    "      <td>7</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25051ac9-25cf-4237-95dd-04bef7508c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f05262-5811-4907-b9d2-3b6fd2c629f6",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedfe11-d6b0-4b3a-98dd-61e3f04e074e",
   "metadata": {},
   "source": [
    "Notice that `.crosstab()` works with numerical data and booleans too. But the column names in the new data frame (the boolean values `True` and `False`) are not very meaningful. We can rename them with `.rename()`.\n",
    "\n",
    "(Make sure, in the previous cell, that you have stored the result of the cross-tabulation in a variable called `intro_by_area`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4338bf-9d7a-4faf-b493-3d8ecd0d16c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_names = {\n",
    "    True: \"Yes\",\n",
    "    False: \"No\"\n",
    "}\n",
    "intro_by_area = intro_by_area.rename(columns=new_names)\n",
    "intro_by_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41b1ca-95fa-4999-85eb-8e0dba668abf",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Now compute the fraction of intro courses per area. This is:\n",
    "\n",
    "$$\n",
    "\\frac{\\rm yes}{({\\rm yes} + {\\rm no})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b1010-9131-4018-bf20-a03b5c6f39c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intro_by_area['frac_intro'] = intro_by_area['Yes'] / (intro_by_area['Yes'] + intro_by_area['No'])\n",
    "intro_by_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4db80e-3eb1-4da6-b39a-c4dff06de36f",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "Reset the index if you want the `area` information to be usable for analysis/plotting/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884c59c-da02-4758-b3fa-4b18bd7f30b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intro_by_area = intro_by_area.reset_index()\n",
    "intro_by_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20827dc0-6ad6-4ff2-a06b-939012d4e764",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeaa969-4e4f-4d5f-bd7f-3f755d42571f",
   "metadata": {},
   "source": [
    "## Computing data based on one or more columns using `.apply()`\n",
    "\n",
    "All these examples involved modifying or creating new columns! In data analysis, we often want to do things to data in our columns for data preparation / cleaning. Sometimes there is missing data we want to re-code, or there is data we want to re-describe or re-classify for our analysis. We can do this with a combination of functions and the `apply()` method. It comes in two flavors:\n",
    "- With a single column (i.e. on a Series)\n",
    "- With multiple columns (i.e. on a DataFrame)\n",
    "\n",
    "### `.apply()` with a single column\n",
    "\n",
    "The `prereqs` column gives a string description of the prerequisites for the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c467452-cc6a-4eba-b761-c2bee02d54f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "courses['prereqs'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34ff50-8618-4702-abb8-105296a72ae8",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Let's say we want to have a `prereqs` column that is sortable. For example:\n",
    "\n",
    "    0 = No prereqs \n",
    "    1 = has prereqs\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "#### Step 1: Define the function you want to apply\n",
    "\n",
    "This cell defines the function and tests it on some sample inputs to check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810b4ca-bd4a-425a-a906-92da6afc9659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: define the function you want to apply\n",
    "def has_prereq(prereq_descr):\n",
    "    \"\"\"\n",
    "    Determine whether a pre-requisite description includes the string \"None\"\n",
    "    \"\"\"\n",
    "    if \"None\" in prereq_descr:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# this should yield 1\n",
    "prereq = \"BMGT301; or instructor permission\" \n",
    "print(f\"With {prereq=!r}: {has_prereq(prereq)=}\")\n",
    "\n",
    "# this should yield 0\n",
    "prereq = \"None\"\n",
    "print(f\"With {prereq=!r}: {has_prereq(prereq)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d35a61-bd3f-46c4-a3e6-27a351c78828",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "#### Step 2: Apply the function to a column\n",
    "\n",
    "We can create a new column called `has_prereqs` to save this information in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339acda2-1ceb-4d5f-8c55-b2b404e3ad44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: apply it to one or more columns\n",
    "\n",
    "# This applies the has_prereq() function to every row in the prereqs column in the courses data frame\n",
    "courses['has_prereqs'] = courses['prereqs'].apply(has_prereq) \n",
    "courses.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59cfdb-786b-4b0f-8dc8-43f1018f3017",
   "metadata": {},
   "source": [
    "<br/><br/><br/>We can crosstab the new column with the previous `is_intro` column to see how many introductory courses have pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617748e-75ee-4f57-9cec-407c25416e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.crosstab(courses['is_intro'], courses['has_prereqs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ed98e-2532-4718-acc4-b98ad9f11d4d",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Interestingly, some courses that are called `\"Introduction ...\"` do have pre-requisites. We can use boolean indexing (in the o to see what are these courses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7f678-01d4-4549-b995-cca302bb8c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "courses[(courses['is_intro'] == True) & (courses['has_prereqs'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02837457-0df9-4e33-9cd6-6e50ea5d5a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def has_intro(descr):\n",
    "    if \"intro\" in descr.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "courses['has_intro'] = courses['description'].apply(has_intro)\n",
    "courses.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78084f4-3cdd-4b51-9380-5a14d2a0408b",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a17c96-359d-4ea3-a310-3ef1ea04f403",
   "metadata": {},
   "source": [
    "#### What's happening under the hood\n",
    "\n",
    "As another example, let's say I want to know how many courses of each level (100-, 200-, 300-level, etc.) we have in each area. We don't have that data in the dataset; at least not explicitly. Fortunately we can make it with some simple programming that you already know how to do! The problem here is, given a code (i.e., data from one column), how do we \"extract\" the area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb4d55-0186-4458-8c43-fa78a47c97b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: define the function\n",
    "def extract_level(code):\n",
    "    \"\"\"\n",
    "    Given a course code (e.g. INST126) extract the course level (100)\n",
    "    \n",
    "    Note that this function assumes course code starts with 4-letter area\n",
    "    \"\"\"\n",
    "    return code[4] + '00'\n",
    "\n",
    "c = \"CMSC250\"\n",
    "extract_level(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3b5c4-5aba-4592-9cf9-7b8d6804d2c4",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Let's see how this works! The `.apply()` function generates a list that is the same length as the input column's number of rows, with a corresponding value for each input \n",
    "\n",
    "(in this case, we have 414 rows in the data frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f045e9-f6e0-4921-b6d1-ed63c7570847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: apply the function\n",
    "courses['level'] = courses['code'].apply(extract_level)\n",
    "courses.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d880a-1ac6-4c55-9b00-9c807bf96914",
   "metadata": {},
   "source": [
    "<br/><br/><br/>This is equivalent to calling `extract_level` repeatedly in a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb29271-5318-4bfb-8faf-084d947575ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(len(courses)):\n",
    "    row = courses.loc[i]\n",
    "    code = row['code']\n",
    "    level = extract_level(code)\n",
    "    print(f\"{i}: code={code}, level={level}\")\n",
    "    tmp.append(level)\n",
    "courses['level'] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be5c57-5868-4373-ae72-762f2426a945",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Another example with the bread basket data frame. Let's extract the hour of the day from the `Time` column and save it in a column called `Hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c67433-47f4-4584-90da-852d38766ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hour(time):\n",
    "    return int(time.split(\":\")[0])\n",
    "\n",
    "bread['Hour'] = bread['Time'].apply(extract_hour)\n",
    "bread.sort_values(by=\"Hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39132e-4b12-44b9-886b-457564a78b7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00452ce6-722c-4fbc-baa9-8028aec3f7bc",
   "metadata": {},
   "source": [
    "#### Step 3: Save the resulting data from the `.apply()` into a new / existing column\n",
    "\n",
    "What if we want to save the results so we can use it later? We can simply assign it to a column, new or existing. \n",
    "\n",
    "Remember, pandas prefers immutability in general (return a new object instead of modifying the object), and sometimes enforces it. \n",
    "\n",
    "With `.apply()`, it's enforced: you can't do it in place, you have to assign the returned series to a new variable if you want it to persist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4e8c7-d10b-4bf0-bd40-db79c6043e87",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f4894-bc47-4835-84c7-20a61ac4a8b1",
   "metadata": {},
   "source": [
    "### `.apply()` with data from multiple columns\n",
    "\n",
    "What if you want to have a way to filter the courses in terms of \"easy entry points\" (i.e., both introductory *and* has no prerequisites)? That might also be interesting to analyze by area to see how many departments offer these easy entry points into the department for students from other departments.\n",
    "\n",
    "Core thing we need to know here is that our `.apply()` will now apply a function that has a **row** as input, not an element of a single column. That way, we can access data from any column in the row: in this case, data from the `is_intro` and `has_prereq` columns. We tell `.apply()` to do this with the `axis` parameter.  We need to pass `axis=1` when we call `.apply()` so it knows to pass a row into the function, not just a single column element. See here for more details: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a4ac1-12c9-4735-b4f5-f9f308c5de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_entry_point function\n",
    "def is_entry_point(row):\n",
    "    \"\"\"\n",
    "    Determine whether a course is an \"entry point\" based on these two conditions:\n",
    "    - It is an \"intro\" course (is_intro = 1)\n",
    "    - It has no prerequisites (has_prere = 1)\n",
    "    \"\"\"\n",
    "    if row['is_intro'] == 1 and row['has_prereqs'] == 0: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e6781-1558-4580-a058-c0526331937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should yield 1\n",
    "test_row = {\n",
    "    'is_intro': 1,\n",
    "    'has_prereqs': 0\n",
    "}\n",
    "print(f\"With test_row = {test_row} is_entry_point(test_row) = {is_entry_point(test_row)}\")\n",
    "      \n",
    "# this should yield 0\n",
    "test_row = {\n",
    "    'is_intro': 1,\n",
    "    'has_prereqs': 1\n",
    "}\n",
    "\n",
    "print(f\"With test_row = {test_row} is_entry_point(test_row) = {is_entry_point(test_row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7d597-6d19-4e31-b3a6-8d97af5bcb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 apply the function (to the whole data frame!) and save the result\n",
    "\n",
    "# need to specify axis=1 to apply it to every row\n",
    "courses['is_entrypoint'] = courses.apply(is_entry_point, axis=1) \n",
    "courses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fcc7e-9c63-48b5-b04a-2fb72d0db51f",
   "metadata": {},
   "source": [
    "<br/><br/><br/>What courses are entry points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d431a-17ca-434f-9e03-a25d1f5c9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses[courses['is_entrypoint'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18602c9f-7178-4db9-9b3f-f503ce874012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "majors = pd.crosstab(courses['area'], courses['is_entrypoint'])\n",
    "majors['frac_entry_points'] = majors[1] / (majors[0] + majors[1])\n",
    "majors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd557f-112b-4d8c-87be-c06cee448df0",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0dbfa-701e-4031-9ac3-05078f1580bf",
   "metadata": {},
   "source": [
    "## The split-apply-combine pattern (with `.groupby()`)\n",
    "\n",
    "We have seen we can &ldquo;reshape&rdquo; a dataframe in various ways: sorting, summarizing, cross-tabulation, etc. Going more deeply on this path of &ldquo;reshaping&rdquo;, we often __want to compute data based on subsets of the data, grouped by some column__. For example, we might want to see how many departments offer &ldquo;easy&rdquo; entry point courses. \n",
    "\n",
    "We can do this with the \"split-apply-combine\" pattern, which is implemented in the `.groupby()` function.\n",
    "\n",
    "Basically, it goes like this:\n",
    "\n",
    "1. **Split** the data into subgroups (e.g., split courses into department subgroups)\n",
    "\n",
    "2. **Apply** some computation on each subgroup (e.g., find number of easy entry points for each department subgroup)\n",
    "\n",
    "3. **Combine** subgroup-computation information into an overall new dataframe that has subgroups as entries\n",
    "\n",
    "More info in this tutorial https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa1696-d5f4-4647-b87a-164b967a87a0",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75056e1c-ee94-457c-a9d9-a492c2e5afd8",
   "metadata": {},
   "source": [
    "### 1. Split \n",
    "\n",
    "We use the `.groupby()` method to split a dataframe into subgroups based on the values of a column\n",
    "\n",
    "Let's split the courses dataframe by area and see how many courses are in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a6b1e-53d9-4a9b-a104-20c24f372a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total number of rows in the data frame\n",
    "print(f\"There are {len(courses)} rows in the data frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d11df2-ad2d-4de2-b789-d9b65e283525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we print the rows in each split\n",
    "for area, area_courses in courses.groupby('area'):\n",
    "    print(f\"There are {len(area_courses)} rows in the data frame for area = {area}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29f4c6-841f-42b0-8beb-77e5f4b07a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the split from the last iteration of the loop: it is a dataframe (notice the area column)\n",
    "area_courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32804372-8e78-404e-bd37-f7f7e525812a",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "#### Split the manual way\n",
    "\n",
    "We use indexing to find subgroups of rows with a given value, then we can apply some summarization statistics, like the average credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de7bb2-27ea-4b4c-8fdd-46a19bc358b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique area values\n",
    "course_areas = courses['area'].value_counts().keys()\n",
    "\n",
    "# iterate through each unique area value\n",
    "for area in course_areas:\n",
    "    # get the subset of the course data that is associated with this area\n",
    "    area_courses = courses[courses['area'] == area]\n",
    "    \n",
    "    # print the number of rows\n",
    "    print(f\"There are {len(area_courses)} rows in the data frame for area = {area}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4bdfb-5e42-4e49-959c-84cbec9ab650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the split from the last iteration of the loop: it is a dataframe (notice the area column)\n",
    "area_courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adf6d3-d9e9-4178-8df1-8265dad8fb74",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Notice also that these numbers correspond to the output of `courses['area'].value_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1eb2a-0b01-4049-81f3-0e25cdfe0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses['area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7339d1-536a-4a57-8258-99b7b129dc44",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f143098-86c2-4462-8cf3-8eaa4d663f93",
   "metadata": {},
   "source": [
    "### Properties of `.groupby()`\n",
    "\n",
    "In general `.groupby()` returns an object of class `DataFrameGroupBy` that performs the split. We use this object to split the courses df into subsets grouped by area. The data frame is first sorted by the column with the groups. The object also works as an _iterator_: we can iterate through the resulting collection of dataframe subsets where each step in the iteration allows us to grab:\n",
    "1. the name of the subset, which is the shared value (in this case area)\n",
    "2. the subset dataframe (here called `area_courses`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633cb43-7c64-4476-acb8-d83d3acf05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses.groupby('area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a519cfe-5acc-4091-9ecd-84fe0ba71347",
   "metadata": {},
   "source": [
    "<br/><br/><br/>On top of supporting iteration, objects returned by `.groupby()` also give you a number of methods/attributes:\n",
    "\n",
    "- `.groupby(...).get(KEY)` &ndash; (_method_) returns the group associated to KEY\n",
    "- `.groupby(...).ngroups` &ndash; (_attribute_) the number of groups\n",
    "- `.groupby(...).groups` &ndash; (_attribute_) a dictionary whose keys are group names and the values the list of corresponding row indices. \n",
    "\n",
    "Here `...` represents the column with the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31079249-c1f9-4b47-b8a9-c773ab2cab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = courses.groupby('area')\n",
    "\n",
    "grouped.get_group('INST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c83c1-1840-4b0e-824c-3b534d8052c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d2377-a054-4847-9cdf-ced7bd009f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5abf58-95f6-4119-84a1-ad5fe4b0d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.groups['INST']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50495366-8a6d-47c9-b845-38eb1f4e9f75",
   "metadata": {},
   "source": [
    "<br/><br/><br/>So yet another way to split the dataframe in groups is to use the `.groups` dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a09376-0f65-4a19-967c-2ad854cc4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = courses.groupby('area')\n",
    "for area in grouped.groups:\n",
    "    idx = grouped.groups[area]  # <--- list of indices of the rows in this area\n",
    "    area_df = courses.loc[idx]  # <--- need to pass list of indices to the .loc[] indexer\n",
    "    print(f\"There are {len(area_courses)} rows in the data frame for area = {area}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a8a557-abba-4a84-acea-f0e6d45ecc76",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c424b16-ed81-4819-92fa-002c95517e71",
   "metadata": {},
   "source": [
    "### 2. Apply and Combine\n",
    "\n",
    "The \"manual\" way: apply and combine into a new dataframe we construct from scratch. First, let's recreate the three columns we built last time using `.apply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7fd2b-916b-45a4-81d8-9e009280e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isintro(title):\n",
    "    return 'Introduction' in title\n",
    "\n",
    "courses['is_intro'] = courses['title'].apply(isintro)\n",
    "\n",
    "\n",
    "def hasprereqs(description):\n",
    "    return 'None' in description\n",
    "\n",
    "courses['has_prereqs'] = courses['prereqs'].apply(hasprereqs)\n",
    "\n",
    "\n",
    "def isentrypoint(row):\n",
    "    if row['is_intro'] == 1 & row['has_prereqs'] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "courses['is_entrypoint'] = courses.apply(isentrypoint, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf48aaf-aae6-4492-ad9e-94f7049eb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to hold the new rows of the new COMBINED dataframe\n",
    "tmp = []\n",
    "\n",
    "# SPLIT the dataframe by area, and iterate through each split\n",
    "for area, areaData in courses.groupby('area'): \n",
    "  \n",
    "    # APPLY operations on the dataframe split\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # count the number of entry point courses in the subarea\n",
    "    num_entrypoints = areaData['is_entrypoint'].sum()\n",
    "    \n",
    "    # count the number of total courses in the subarea\n",
    "    num_classes = len(areaData)\n",
    "    \n",
    "\n",
    "    # COMBINE the resulting subcomputation into a new dataset\n",
    "    # -------------------------------------------------------\n",
    "    entry = {\n",
    "      'area': area, # each row is an area\n",
    "      'num_entrypoints': num_entrypoints, \n",
    "      'num_classes': num_classes,\n",
    "    }\n",
    "    tmp.append(entry) \n",
    "    \n",
    "# convert the list of new entries into a dataframe\n",
    "entry_courses_by_area = pd.DataFrame(tmp)\n",
    "entry_courses_by_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d19603-d46a-4388-81f8-de22da5b6065",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Another example: what is busiest hour of the day at the restaurant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c7345-bd7d-440e-b832-c0e3ba214cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207ebbe-6a82-46d9-b7ca-eb35cb0dbaa0",
   "metadata": {},
   "source": [
    "<br/><br/><br/>First, we use `.apply()` again to extract the hour of the transaction from the `Time` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af461b-196c-464b-b142-f22d875d32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gethour(time):\n",
    "    return int(time.split(\":\")[0])\n",
    "\n",
    "bread['hour'] = bread['Time'].apply(gethour)\n",
    "bread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde8983-9a33-4603-b59a-d0f4eccb5cd2",
   "metadata": {},
   "source": [
    "<br/><br/><br/>The use split-apply-combine pattern again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029b470-579a-4987-9cf8-32afaf2a5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list to hold the new rows of the new COMBINED dataframe\n",
    "tmp = []\n",
    "\n",
    "# SPLIT the dataframe by area, and iterate through each split\n",
    "for hour, hour_df in bread.groupby('hour'):\n",
    "  \n",
    "    # APPLY operations on the dataframe split\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # count the number of entry point courses in the subarea\n",
    "    num_transactions = len(hour_df)    \n",
    "\n",
    "    # COMBINE the resulting subcomputation into a new dataset\n",
    "    # -------------------------------------------------------\n",
    "    entry = {\n",
    "      'hour': hour, # each row is an hour\n",
    "      'num_transactions': num_transactions, \n",
    "    }\n",
    "    tmp.append(entry) \n",
    "    \n",
    "# convert the list of new entries into a dataframe\n",
    "transactions_by_hour = pd.DataFrame(tmp)\n",
    "transactions_by_hour    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e68d6-9013-4e4b-ac55-a14dbd9a03bd",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62439340-6d67-478f-8628-5f1f41f801b1",
   "metadata": {},
   "source": [
    "#### Named aggregation: Shortcut apply-combine with `.groupby()` + `.agg()`\n",
    "\n",
    "To make `.groupby()` more powerful, we tack on the `.agg()` function to it to tell pandas to *aggregate* particular columns in particular ways (e.g., count the number of entry point courses in a given department, vs. give an average *proportion* of classes that are entry points). This is also called [named aggregation](https://pandas.pydata.org/docs/user_guide/groupby.html#named-aggregation) in the pandas official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef141ee-5d09-4997-95c0-00c487f12c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT by area\n",
    "courses.groupby(\"area\", as_index=False).agg(\n",
    "    # APPLY these computations and COMBINE into a new data frame using .agg\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    # 1: apply `.sum()` to the `is_entrypoint` column of each subgroup\n",
    "    num_entrypoints=('is_entrypoint', \"sum\"), \n",
    "    # 2: apply `.count() to the `area` column of each subgroup (similar to taking len() of subgroup)\n",
    "    num_classes=('area', \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b25ac-7bf9-4f8b-b223-aa6be23b3781",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Same with restaurant transactions data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c70ab-20de-4b10-8f8d-993067f462e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bread.groupby(\"hour\", as_index=False).agg(\n",
    "    num_transactions=(\"Time\", \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3816aaa-2614-4aa9-905c-285af6e42df2",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Named aggregations is a relatively newer feature in Pandas. The old school method is much more convoluted. You may still see if sometimes on StackOverflow or other websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9681a-71cb-4330-b9de-cd44c5567eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bread.groupby(\"hour\", as_index=False)[['Time']].count().rename(columns={'Time': 'num_transactions'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3c89a-e51b-4ec4-a842-99cdeb4f33b8",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b095d-1193-40da-a08c-3c61472e66a9",
   "metadata": {},
   "source": [
    "## Anatomy of combining `.groupby()` with `.agg()` via named aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad3c32-15aa-43d7-ba97-e608427a3a2a",
   "metadata": {},
   "source": [
    "<img src=\"https://terpconnect.umd.edu/~gciampag/INST126/images/groupby.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88660d47-b002-4149-b027-2860dddcaa7e",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac34e41-02d5-4b64-bca4-9ea48ca2e35f",
   "metadata": {},
   "source": [
    "### Using groupby for further analysis\n",
    "\n",
    "Sometimes we want to take the result of the split-apply-combined data frame and do further analysis on it. Recall we said an entry point course (introduction + no prereqs) is an &ldquo;easy&rdquo; to get a sense of what an area is like.  Areas with more entry points are more &ldquo;open&rdquo; to people wishing to change major for example. What areas are more open? Now that we now how may entry points courses are there per area, and also how many courses in total there are, we could compute an &ldquo;openness&rdquo; score.\n",
    "\n",
    "Here is the aggregated data frame we built before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8374f14-860a-4454-a7aa-f99793def5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_courses_by_area = courses.groupby(\"area\", as_index=False).agg(\n",
    "    num_entrypoints=('is_entrypoint', \"sum\"), \n",
    "    num_classes=('area', \"count\")\n",
    ")\n",
    "entry_courses_by_area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e710b2c-e131-48e0-8c0c-1bd3f89b9947",
   "metadata": {},
   "source": [
    "<br/><br/><br/>Let's now compute the proportion of entry point classes, as a proxy for &ldquo;openness&rdquo;, and finally let's sort by that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ce9ca-3d3f-4e53-8f5d-0f9626228730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: define the function\n",
    "def openness(row):\n",
    "    return row['num_entrypoints'] / row['num_classes']\n",
    "\n",
    "# step 2: apply the function and save the results\n",
    "entry_courses_by_area['openness'] = entry_courses_by_area.apply(openness, axis=1)\n",
    "\n",
    "# step 3: sort by openness and reset index (passing ignore_index=True argument)\n",
    "entry_courses_by_area.sort_values(by=\"openness\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1dcba-b7be-4581-8e20-6a2ac544b699",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fc8e8-5dea-488d-8ea5-b1cd9c1a14d5",
   "metadata": {},
   "source": [
    "### What can you aggregate?\n",
    "\n",
    "<table class=\"colwidths-given table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 20%\">\n",
    "<col style=\"width: 80%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Function</p></th>\n",
    "<th class=\"head\"><p>Description</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">mean()</span></code></p></td>\n",
    "<td><p>Compute mean of groups</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">sum()</span></code></p></td>\n",
    "<td><p>Compute sum of group values</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">size()</span></code></p></td>\n",
    "<td><p>Compute group sizes</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">count()</span></code></p></td>\n",
    "<td><p>Compute count of group</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">std()</span></code></p></td>\n",
    "<td><p>Standard deviation of groups</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">var()</span></code></p></td>\n",
    "<td><p>Compute variance of groups</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">sem()</span></code></p></td>\n",
    "<td><p>Standard error of the mean of groups</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">describe()</span></code></p></td>\n",
    "<td><p>Generates descriptive statistics</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">first()</span></code></p></td>\n",
    "<td><p>Compute first of group values</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">last()</span></code></p></td>\n",
    "<td><p>Compute last of group values</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">nth()</span></code></p></td>\n",
    "<td><p>Take nth value, or a subset if n is a list</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">min()</span></code></p></td>\n",
    "<td><p>Compute min of group values</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">max()</span></code></p></td>\n",
    "<td><p>Compute max of group values</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e24e0-cd18-4a66-8727-b15417f44686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getname(coaches):\n",
    "    try:\n",
    "        return \" \".join(coaches.split(\" \")[:2])\n",
    "    except AttributeError:\n",
    "        return \"No Coach\"\n",
    "\n",
    "ncaa['coach'] = ncaa['coaches'].apply(getname)\n",
    "\n",
    "ncaa.groupby('coach', as_index=False).agg(\n",
    "    best_wl=('wl', 'max')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa5d7c-49ae-4102-9fb3-7da746f9711d",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e4a99-8a67-4c7d-be7a-a8230aab5c32",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "<img src=\"https://terpconnect.umd.edu/~gciampag/INST126/images/putting-together.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed06bb-7264-4949-a7f2-827dfdd98630",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df42ab9-5059-4273-9ed7-fc20586945da",
   "metadata": {},
   "source": [
    "## Reminder: More resources\n",
    "\n",
    "* The pandas website is decent place to start: https://pandas.pydata.org/\n",
    "* This \"cheat sheet\" is also a really helpful guide to more common operations that you may run into later: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "* There are also many blogs that are helpful, like towardsdatascience.com\n",
    "* The cool thing about pandas and data analysis in python is that many people share notebooks that you can inspect / learn from / adapt code for your own projects (just like mine!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2e85e-f56a-4c83-990e-20cb0a41623d",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6572c14-ba32-4a93-bc2e-fbcf49839379",
   "metadata": {},
   "source": [
    "## EXTRA: Plotting\n",
    "\n",
    "The main library for plotting in Python is `matplotlib`. You can learn that library later. It has lots of fine-grained controls. For now, you can use pandas \"wrapper\" over matplotlib (basically calling matplotlib from inside pandas), which is a bit easier to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2df68-7594-4ac4-a143-380373eb6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot openness by area\n",
    "entry_courses_by_area.sort_values(by='openness', ascending=False).plot(\n",
    "    x=\"area\", \n",
    "    y=\"openness\", \n",
    "    kind='bar', \n",
    "    xlabel=\"Major\", \n",
    "    ylabel=\"Proportion of entry point classes\",\n",
    "    title=\"Average openness by major\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9c039-55b2-4094-8c86-1c39707c088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_courses_by_area.sort_values(by=\"num_classes\", ascending=False).plot(\n",
    "    x=\"area\", \n",
    "    y=\"num_classes\", \n",
    "    kind=\"bar\",\n",
    "    xlabel=\"Major\",\n",
    "    ylabel=\"Number of classes\",\n",
    "    title=\"Number of courses by major\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9a64e-2531-498a-9e61-7ce0edc78051",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d817f-f541-4a3e-90ed-68ef39689737",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac1689-d634-41b5-aaf5-3680039e95a0",
   "metadata": {},
   "source": [
    "## Coding Challenge \\#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4a2ec-9b19-47f2-adfd-a8224558d567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "bread = pd.read_csv('data/BreadBasket_DMS.csv')  # Note: on Windows use data\\BreadBasket_DMS.csv\n",
    "bread['Item'].value_counts()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a42c6-ad6c-43c2-9134-267dde27b12e",
   "metadata": {},
   "source": [
    "## Coding Challenge \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7df36-9e2a-47a4-a13b-4815c9216b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def is_intro(title):\n",
    "    \"\"\" \n",
    "    Determine whether a course title has the word \"Introduction\" or not in it\n",
    "    \"\"\"\n",
    "    if \"Introduction\" in title:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "# apply the is_intro function to the title column in courses\n",
    "# and save the results in the is_intro column in courses\n",
    "courses['is_intro'] = courses['title'].apply(is_intro)\n",
    "\n",
    "# cross-tabulate area and is_intro\n",
    "intro_by_area = pd.crosstab(courses[\"area\"], courses[\"is_intro\"])\n",
    "intro_by_area\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
